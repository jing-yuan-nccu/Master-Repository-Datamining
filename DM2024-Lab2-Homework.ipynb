{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b18a657-b1e6-481d-bdc2-04d8f1ef8c1b",
   "metadata": {},
   "source": [
    "### Student Information \n",
    "Name: 林靖淵<br>\n",
    "Student ID: 113356040<br>\n",
    "GitHub ID: https://github.com/jing-yuan-nccu<br>\n",
    "Kaggle name: jingyaun_nccu<br>\n",
    "Kaggle private scoreboard snapshot: <br>\n",
    "***\n",
    "### Instructions\n",
    "1. First: This part is worth 30% of your grade. Do the take home exercises in the DM2024-Lab2-master Repo. You may need to copy some cells from the Lab notebook to this notebook.\n",
    "2. Second: This part is worth 30% of your grade. Participate in the in-class Kaggle Competition regarding Emotion Recognition on Twitter by this link: https://www.kaggle.com/competitions/dm-2024-isa-5810-lab-2-homework. The scoring will be given according to your place\n",
    "in the Private Leaderboard ranking:\n",
    "Bottom 40%: Get 20% of the 30% available for this section.\n",
    "Top 41% - 100%: Get (0.6N + 1 - x) / (0.6N) * 10 + 20 points, where N is the total number of participants, and x is your rank. (ie. If there are 100 participants and you rank 3rd your score will be (0.6 * 100 + 1 - 3) / (0.6 * 100) * 10 + 20 = 29.67% out of 30%.)\n",
    "Submit your last submission BEFORE the deadline (Nov. 26th, 11:59 pm, Tuesday). Make sure to take a screenshot of your position at the end of the competition and store it as '''pic0.png''' under the img folder of this repository and rerun the cell Student Information.\n",
    "3. Third: This part is worth 30% of your grade. A report of your work developing the model for the competition (You can use code and comment on it). This report should include what your preprocessing steps, the feature engineering steps and an explanation of your model.\n",
    "You can also mention different things you tried and insights you gained.\n",
    "4. Fourth: This part is worth 10% of your grade. It's hard for us to follow if your code is messy :'(, so please tidy up your notebook.\n",
    "Upload your files to your repository then submit the link to it on the corresponding e-learn assignment.\n",
    "Make sure to commit and save your changes to your repository BEFORE the deadline (Nov. 26th, 11:59 pm, Tuesday)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d205377-48d6-434a-a996-e92dbdb315af",
   "metadata": {},
   "source": [
    "## First Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d17b83-952f-41a5-b752-bf999f2f7cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "### training data\n",
    "anger_train = pd.read_csv(\"data/semeval/train/anger-ratings-0to1.train.txt\",\n",
    "                         sep=\"\\t\", header=None,names=[\"id\", \"text\", \"emotion\", \"intensity\"])\n",
    "sadness_train = pd.read_csv(\"data/semeval/train/sadness-ratings-0to1.train.txt\",\n",
    "                         sep=\"\\t\", header=None, names=[\"id\", \"text\", \"emotion\", \"intensity\"])\n",
    "fear_train = pd.read_csv(\"data/semeval/train/fear-ratings-0to1.train.txt\",\n",
    "                         sep=\"\\t\", header=None, names=[\"id\", \"text\", \"emotion\", \"intensity\"])\n",
    "joy_train = pd.read_csv(\"data/semeval/train/joy-ratings-0to1.train.txt\",\n",
    "                         sep=\"\\t\", header=None, names=[\"id\", \"text\", \"emotion\", \"intensity\"])\n",
    "# combine 4 sub-dataset\n",
    "train_df = pd.concat([anger_train, fear_train, joy_train, sadness_train], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0901f0f3-42c3-4f27-afbf-2a8ab5bea8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "### testing data\n",
    "anger_test = pd.read_csv(\"data/semeval/dev/anger-ratings-0to1.dev.gold.txt\",\n",
    "                         sep=\"\\t\", header=None, names=[\"id\", \"text\", \"emotion\", \"intensity\"])\n",
    "sadness_test = pd.read_csv(\"data/semeval/dev/sadness-ratings-0to1.dev.gold.txt\",\n",
    "                         sep=\"\\t\", header=None, names=[\"id\", \"text\", \"emotion\", \"intensity\"])\n",
    "fear_test = pd.read_csv(\"data/semeval/dev/fear-ratings-0to1.dev.gold.txt\",\n",
    "                         sep=\"\\t\", header=None, names=[\"id\", \"text\", \"emotion\", \"intensity\"])\n",
    "joy_test = pd.read_csv(\"data/semeval/dev/joy-ratings-0to1.dev.gold.txt\",\n",
    "                         sep=\"\\t\", header=None, names=[\"id\", \"text\", \"emotion\", \"intensity\"])\n",
    "\n",
    "# combine 4 sub-dataset\n",
    "test_df = pd.concat([anger_test, fear_test, joy_test, sadness_test], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0af002c-9fb2-4030-903b-5ecb96ca5594",
   "metadata": {},
   "source": [
    "---\n",
    "### ** >>> Exercise 1 (Take home): **  \n",
    "Plot word frequency for Top 30 words in both train and test dataset. (Hint: refer to DM lab 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5fadf0-3be2-49e2-8d7f-068486bdee2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer\n",
    "import nltk\n",
    "import helpers.data_mining_helpers as dmh\n",
    "\n",
    "# do word seperation\n",
    "X_df = pd.concat([train_df, test_df], ignore_index=True)\n",
    "nltk.download('punkt_tab')\n",
    "X_df['unigrams'] = X_df['text'].apply(lambda x: dmh.tokenize_text(x))\n",
    "\n",
    "# convert each unigram to vertor form\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "X_counts = count_vect.fit_transform(X_df.text) #learn the vocabulary and return document-term matrix\n",
    "print(X_counts.shape)\n",
    "\n",
    "# concate terms and frenquencies together in a dataframe\n",
    "plot_x = [\"term_\"+str(i) for i in count_vect.get_feature_names_out()[:]]\n",
    "plot_y = [\"id_\"+ str(i) for i in list(X_df.index)[:]]\n",
    "plot_z = X_counts[:, :].toarray() #X_counts[how many documents, how many terms]\n",
    "\n",
    "df_todraw = pd.DataFrame(plot_z, columns = plot_x, index = plot_y)\n",
    "\n",
    "# sum and sort\n",
    "df_sum = df_todraw.sum(axis=0)\n",
    "df_sum_sorted = df_sum.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26133719-8997-46d8-8be1-70692c210c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "# plot top 30 words frequency \n",
    "df_plot = pd.DataFrame({\n",
    "    'Term': df_sum_sorted.index[:30],      # Term names as index\n",
    "    'Frequency': df_sum_sorted.values[:30] # Frequencies as values\n",
    "})\n",
    "\n",
    "fig = px.bar(df_plot, x=\"Term\", y='Frequency',color=\"Frequency\",  # Assign color based on frequency values\n",
    "             color_continuous_scale=\"Viridis\")\n",
    "\n",
    "# Update layout (rotating labels, sizing, etc.)\n",
    "fig.update_layout(\n",
    "    xaxis_tickangle=90,  # Rotate x-axis labels\n",
    "    height=500,  # Set figure height\n",
    "    width=1200,  # Set figure width\n",
    "    title=\"Term Frequencies\",\n",
    "    xaxis_title=\"Terms\",\n",
    "    yaxis_title=\"Frequency\"\n",
    ")\n",
    "\n",
    "# Show the figure\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a328133b-3e88-4e12-8fc8-9c81daafb74b",
   "metadata": {},
   "source": [
    "---\n",
    "### ** >>> Exercise 2 (Take home): **  \n",
    "Generate an embedding using the TF-IDF vectorizer instead of th BOW one with 1000 features and show the feature names for features [100:110]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3497c5-66d6-41fe-a914-8ee6d57e08ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer here\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_features=1000,  # Limit the number of features (optional)\n",
    "    # stop_words='english',  # Remove stopwords (optional)\n",
    "    ngram_range=(1, 1)  # Use unigrams only; adjust for bigrams/trigrams if needed\n",
    ")\n",
    "\n",
    "# Fit and transform the corpus to generate TF-IDF matrix\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(train_df['text'])\n",
    "\n",
    "# Convert the TF-IDF matrix to a DataFrame for better readability\n",
    "tfidf_df = pd.DataFrame(\n",
    "    tfidf_matrix.toarray(),  # Convert sparse matrix to dense array\n",
    "    columns=tfidf_vectorizer.get_feature_names_out()  # Feature names (words)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e027a30-fd97-40b7-8d3c-2faf4b0b52c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names_1000 = tfidf_vectorizer.get_feature_names_out()\n",
    "feature_names_1000[100:110]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6347a69-07a7-471c-bffd-7ac00ac76f5e",
   "metadata": {},
   "source": [
    "---\n",
    "### ** >>> Exercise 3 (Take home): **  \n",
    "Can you interpret the results above? What do they mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7730ea-4971-4a51-919c-c94751a358b0",
   "metadata": {},
   "source": [
    "***Answer***<br>\n",
    "The diagonal values (57, 76, 56, 47) represent the number of instances correctly predicted for each emotion, while the other numbers indicate the misclassified instances where the column emotion was mistakenly predicted as the row emotion. From the confusion matrix, it can be observed that the emotion \"fear\" is more prone to being misclassified as other emotions. <br>Notably, among the four emotions, there is a higher rate of mutual misclassification between \"anger\" and \"fear.\" This could be due to the difficulty in distinguishing between these two emotions, as well as between \"fear\" and \"sadness.\" It is also possible that the emotion \"fear\" sometimes exhibits characteristics of anger or sadness, leading to these misclassifications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962d679f-a447-4e3a-b937-57acbca181db",
   "metadata": {},
   "source": [
    "---\n",
    "### ** >>> Exercise 4 (Take home): **  \n",
    "Build a model using a ```Naive Bayes``` model and train it. What are the testing results? \n",
    "\n",
    "*Reference*: https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bbf022-d2d3-4032-9347-0b7833734754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer here\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Load your dataset (example uses pandas DataFrame)\n",
    "import pandas as pd\n",
    "\n",
    "## build DecisionTree model\n",
    "DT_model = MultinomialNB()\n",
    "\n",
    "## training!\n",
    "DT_model = DT_model.fit(X_train, y_train)\n",
    "\n",
    "## predict!\n",
    "y_train_pred = DT_model.predict(X_train)\n",
    "y_test_pred = DT_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7a5592-d906-4615-babf-ada531bb8239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "## predict!\n",
    "y_train_pred = DT_model.predict(X_train)\n",
    "y_test_pred = DT_model.predict(X_test)\n",
    "\n",
    "print(classification_report(y_true=y_test, y_pred=y_test_pred))\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b30bbe-d7fb-4b93-b397-b10d2c3a0e9a",
   "metadata": {},
   "source": [
    "---\n",
    "### ** >>> Exercise 5 (Take home): **  \n",
    "\n",
    "How do the results from the Naive Bayes model and the Decision Tree model compare? How do you interpret these differences? Use the theoretical background covered in class to try and explain these differences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e78f912-0219-4cdd-9839-c6d43d45d121",
   "metadata": {},
   "source": [
    "***Answer***<br>\n",
    "Based on the comparison, it can be observed that Naive Bayes is relatively weaker in distinguishing angry emotions. Additionally, there is some similarity between fear and sadness or anger, leading to occasional misclassifications. More prominent errors (i.e., a higher number of misclassifications) are present across all emotional categories except for joy. Overall, Naive Bayes is generally recognized to produce better results because it excels at handling high-dimensional textual data. However, its performance in distinguishing anger and sadness is weaker, possibly due to the assumption of feature independence in Naive Bayes. Emotions like anger and sadness require the interdependence of features for more accurate classification. On the other hand, fear-related emotions tend to achieve better results under the assumption of feature independence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04002856-1c64-4d2b-ac64-35ce612ce487",
   "metadata": {},
   "source": [
    "---\n",
    "### ** >>> Exercise 6 (Take home): **  \n",
    "\n",
    "Plot the Training and Validation Accuracy and Loss (different plots), just like the images below.(Note: the pictures below are an example from a different model). How to interpret the graphs you got? How are they related to the concept of overfitting/underfitting covered in class?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88f0928-4a60-47ed-951d-3d54bc930fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer here\n",
    "df = pd.DataFrame(training_log)\n",
    "plt.figure(figsize=(14, 6))\n",
    "# Subplot 1: Accuracy\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(df['epoch'], df['accuracy'], label='Accuracy', marker='o')\n",
    "plt.plot(df['epoch'], df['val_accuracy'], label='Validation Accuracy', marker='o')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "# Subplot 2: Loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(df['epoch'], df['loss'], label='Loss', marker='o')\n",
    "plt.plot(df['epoch'], df['val_loss'], label='Validation Loss', marker='o')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "# Show the plots\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4570e327-e8ec-40ce-8f74-493c90e5b1da",
   "metadata": {},
   "source": [
    "***Answer***<br>\n",
    "Observing the Model Accuracy and Model Loss plots can help identify when the model starts to overfit. From the Accuracy plot, it can be seen that before epoch 5, the growth trend of the validation accuracy begins to plateau. At this point, only the training accuracy continues to increase, while the validation accuracy remains stable. This indicates that the model starts to overfit around epoch 5. Similarly, in the Model Loss plot, the validation loss starts to increase sharply at epoch 5, whereas the training loss continues to decrease. This is another clear sign of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba70889d-4c8e-40e7-9a73-61f273e3311d",
   "metadata": {},
   "source": [
    "---\n",
    "### ** >>> Exercise 7 (Take home): **  \n",
    "\n",
    "Now, we have the word vectors, but our input data is a sequence of words (or say sentence). \n",
    "How can we utilize these \"word\" vectors to represent the sentence data and train our model?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946d0f49-611a-4f7d-b584-948c5a45d595",
   "metadata": {},
   "source": [
    "***Answer***<br>\n",
    "We can calculate the average of words in a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee917079-d078-405f-9681-db349abfbaa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_vector(sentence, model):\n",
    "    # Filter out words not in the model's vocabulary\n",
    "    words = [word for word in sentence if word in model.wv] \n",
    "    if not words:\n",
    "        return np.zeros(model.vector_size)\n",
    "    # Compute the mean of the word vectors\n",
    "    return np.mean(model.wv[words], axis=0)\n",
    "\n",
    "train_df[\"sv\"] = train_df[\"text_tokenized\"].apply(lambda x: sentence_vector(x,word2vec_model))\n",
    "train_df[\"sv\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed645190-a56a-45be-9745-a65f9c07d729",
   "metadata": {},
   "source": [
    "---\n",
    "### ** >>> Exercise 8 (Take home): **  \n",
    "\n",
    "Generate a t-SNE and UMAP visualization to show the 15 words most related to the words \"angry\", \"happy\", \"sad\", \"fear\" (60 words total). Compare the differences between both graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6c81ad-5aa6-4a58-bd09-1c24242a5ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer here\n",
    "# Data preparation\n",
    "word_list = ['angry', 'happy', 'sad', 'fear']\n",
    "\n",
    "topn = 15\n",
    "angry_words = ['angry'] + [word_ for word_, sim_ in w2v_google_model.most_similar('angry', topn=topn)]        \n",
    "happy_words = ['happy'] + [word_ for word_, sim_ in w2v_google_model.most_similar('happy', topn=topn)]\n",
    "sad_words = ['sad'] + [word_ for word_, sim_ in w2v_google_model.most_similar('sad', topn=topn)]        \n",
    "fear_words = ['fear'] + [word_ for word_, sim_ in w2v_google_model.most_similar('fear', topn=topn)]        \n",
    "\n",
    "target_words = angry_words + happy_words + sad_words + fear_words\n",
    "cn = topn + 1\n",
    "color = ['r'] * cn + ['y'] * cn + ['b'] * cn + ['g'] * cn\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "import umap.umap_ as umap\n",
    "\n",
    "## w2v model\n",
    "model = w2v_google_model\n",
    "\n",
    "## prepare training word vectors\n",
    "size = 200\n",
    "target_size = len(target_words)\n",
    "all_word = list(model.index_to_key)\n",
    "word_train = target_words + all_word[:size]\n",
    "X_train = model[word_train]\n",
    "\n",
    "## UMAP model\n",
    "umap_model = umap.UMAP(n_components=2, metric='cosine', random_state=28)\n",
    "\n",
    "## training\n",
    "X_umap = umap_model.fit_transform(X_train)\n",
    "\n",
    "## t-SNE model\n",
    "tsne = TSNE(n_components=2, metric='cosine', random_state=28)\n",
    "\n",
    "## training\n",
    "X_tsne = tsne.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d61a09-311f-4abe-9810-e022e63239e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the result with two subplots in one canvas\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 7.5), dpi=115)\n",
    "\n",
    "# First subplot\n",
    "axes[0].scatter(X_tsne[:target_size, 0], X_tsne[:target_size, 1], c=color)\n",
    "axes[0].set_title('Plot 1: t-SNE Visualization')\n",
    "for label, x, y in zip(target_words, X_tsne[:target_size, 0], X_tsne[:target_size, 1]):\n",
    "    axes[0].annotate(label, xy=(x, y), xytext=(0, 0), textcoords='offset points')\n",
    "\n",
    "# Second subplot (can use different data or the same for demonstration)\n",
    "axes[1].scatter(X_umap[:target_size, 0], X_umap[:target_size, 1], c=color)\n",
    "axes[1].set_title('Plot 2: UMAP Visualization with Labels')\n",
    "for label, x, y in zip(target_words, X_umap[:target_size, 0], X_umap[:target_size, 1]):\n",
    "    axes[1].annotate(label, xy=(x, y), xytext=(0, 0), textcoords='offset points')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8277e6f7-aa00-4d3f-8829-000f6585f3b1",
   "metadata": {},
   "source": [
    "***Answer***<br>\n",
    "Observing the two plots, it can be seen that the UMAP results are more compact, while the t-SNE results are more dispersed. If all points are displayed in the same color, UMAP's results make it easier to distinguish which points form a group compared to t-SNE. This indirectly suggests that t-SNE may overlook global relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313d8145-a850-4ee9-bd26-618b055dea96",
   "metadata": {},
   "source": [
    "### ** >>> Exercise 9 (Take home): **  \n",
    "\n",
    "You noticed there is a **role** parameter inside the ollama.chat function, investigate what other roles there can be inside the function and what do they do. Give an example of a prompt using another role in additional to the **user** one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c45e65-3e20-4f83-815a-6020355342bc",
   "metadata": {},
   "source": [
    "***Answer***<br>\n",
    "There are three roles can be inside the function. The other two roles are \"system\" and \"assistant\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860807b5-2036-4d68-8c90-5841b37b37bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = ollama.chat(\n",
    "            model='llava-phi3',\n",
    "            messages=[{\n",
    "                'role': 'system',\n",
    "                'content': 'Summarize this text. Chroma is a vector database that stores the document embeddings.It creates a vector space from the text chunks (splits) and their corresponding embeddings (embeddings).This vector space allows for similarity-based comparisons, enabling efficient document retrieval.'\n",
    "            }]\n",
    "        )\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815f9db8-839a-4ea9-a96a-b12d1c1e6ffe",
   "metadata": {},
   "source": [
    "### ** >>> Exercise 10 (Take home): **  \n",
    "\n",
    "Try asking the model with one image of your choosing. Is the description accurate? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a779f19-87ab-44bf-b83a-a48047a5094a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer here\n",
    "response4 = ollama.chat(model='llava-phi3', messages=[\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': 'What is this image about?',\n",
    "        'images': ['./pics/example3.jpg'] #Image with the cat\n",
    "    },\n",
    "])\n",
    "\n",
    "display(Markdown(response4['message']['content']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e81560a-5248-4c45-972b-13f974759af3",
   "metadata": {},
   "source": [
    "***Answer***<br>\n",
    "To be honest, the description accurate is good. It's is far from my expection. I think it is because I give it a simple picture with only one objects. Also, from the response, we can tell the model trainer gave the generation a structure to follow. Maybe that why the description is good."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e9124c-ed16-43d1-86ae-b9c697e41fc8",
   "metadata": {},
   "source": [
    "\n",
    "### ** >>> Exercise 11 (Take home): **  \n",
    "\n",
    "Try to modify the code to make it accept **three URLs**, or **three text documents** of your choosing. After modifying it, make **three prompts/questions** with information that can be found in each of the documents/urls, **compare the accuracy of the response** with the actual answer. Investigate and discuss the advantages and disadvantages of RAG systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e697838-bffc-4060-8c7b-19795ce4b19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer here\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OllamaEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# Function to load, split, and retrieve documents from multiple URLs\n",
    "def load_and_retrieve_multiple_docs(urls):\n",
    "    all_splits = []  # To store chunks from all URLs\n",
    "\n",
    "    # Loop through each URL\n",
    "    for url in urls:\n",
    "        loader = WebBaseLoader(\n",
    "            web_paths=(url,),\n",
    "            bs_kwargs=dict()\n",
    "        )\n",
    "        docs = loader.load()  # Load documents from the URL\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "        splits = text_splitter.split_documents(docs)  # Split into chunks\n",
    "        all_splits.extend(splits)  # Add chunks to the combined list\n",
    "\n",
    "    # Generate embeddings and create vector store\n",
    "    embeddings = OllamaEmbeddings(model=llm_model)\n",
    "    vectorstore = Chroma.from_documents(documents=all_splits, embedding=embeddings)\n",
    "\n",
    "    return vectorstore.as_retriever()\n",
    "\n",
    "# List of URLs to process\n",
    "urls = [\n",
    "    \"https://www.ibm.com/topics/large-language-models\",\n",
    "    \"https://www.ibm.com/topics/data-mining\",\n",
    "    \"https://www.ibm.com/topics/chatbots\"\n",
    "]\n",
    "\n",
    "# Create the retriever\n",
    "retriever = load_and_retrieve_multiple_docs(urls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56b0e88-8b69-4d38-9004-6e79001e12a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs) #Format the retrieved docs in an orderly manner for prompting\n",
    "\n",
    "# Define the Ollama LLM function\n",
    "def ollama_llm(question, context):\n",
    "    formatted_prompt = f\"Question: {question}\\n\\nContext: {context}\"\n",
    "    response = ollama.chat(model='llama3.2', messages=[{'role': 'user', 'content': formatted_prompt}])\n",
    "    return response['message']['content']\n",
    "\n",
    "# Define the RAG chain\n",
    "def rag_chain(question):\n",
    "    retrieved_docs = retriever.invoke(question)\n",
    "    formatted_context = format_docs(retrieved_docs)\n",
    "    return ollama_llm(question, formatted_context)\n",
    "\n",
    "questions = [\n",
    "    \"What is LLMs?\",\n",
    "    \"What is data mining?\",\n",
    "    \"What is chatbot?\"\n",
    "]\n",
    "# Use the RAG chain\n",
    "for q in questions:\n",
    "    result = rag_chain(q)\n",
    "    display(Markdown(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6ec74c-8e8d-434c-a4c3-4ba603c13d11",
   "metadata": {},
   "source": [
    "***Answer***<br>\n",
    "RAG has strong scalability, allowing the model to connect to external resources and provide responses that are more up-to-date. However, this adds additional processes, which on one hand, consume computational resources, and on the other hand, may lead to a decline in response quality if there are issues in the RAG process. For example, retrieving incorrect documents can negatively impact the final output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40e2945-35e4-43fd-907a-e7edaeb25819",
   "metadata": {},
   "source": [
    "### ** >>> Exercise 12 (Take home): **\n",
    "\n",
    "Follow Exercise 6 again and Plot the Training and Validation Accuracy and Loss for the results of this Neural Network. Compare the results of both KNN and the NN we just implemented. Discuss about why we obtained these results with the LLM Embeddings compared to the results of the other models implemented in this Lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa1ad32-a75f-4643-8c31-57583862f0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer here\n",
    "df = pd.DataFrame(training_log)\n",
    "plt.figure(figsize=(14, 6))\n",
    "# Subplot 1: Accuracy\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(df['epoch'], df['accuracy'], label='Accuracy', marker='o')\n",
    "plt.plot(df['epoch'], df['val_accuracy'], label='Validation Accuracy', marker='o')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "# Subplot 2: Loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(df['epoch'], df['loss'], label='Loss', marker='o')\n",
    "plt.plot(df['epoch'], df['val_loss'], label='Validation Loss', marker='o')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "# Show the plots\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439bd269-2c6d-46d8-8c93-f01fb94b726d",
   "metadata": {},
   "source": [
    "***Answer***<br>\n",
    "It can be observed that the results using embeddings are worse than those from the previous model. Whether using a neural network or the KNN method, the performance with embeddings is inferior to the previous model. This might indicate that embeddings are not well-suited for emotion detection, or that the sentences may require more preprocessing to help the model capture the key information in the text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f37a027-ff93-4aca-b3bd-68f5804920b9",
   "metadata": {},
   "source": [
    "### ** >>> Exercise 13 (Take home): **\n",
    "\n",
    "Compare and discuss the results of the zero-shot, 1-shot and 5-shot classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f191623e-71fc-4e4d-b541-41d1b0be7d40",
   "metadata": {},
   "source": [
    "***Answer***<br>\n",
    "It can be observed that among 0-shot, 1-shot, and 5-shot scenarios, 1-shot performs the best. Compared to 0-shot, providing an example in the prompt indeed helps the LLM better understand the task at hand. However, excessive prompting does not seem to yield better results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58fa36e-6b70-4021-a621-74953e0f10d4",
   "metadata": {},
   "source": [
    "## Third"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7468f983-0396-45af-b6b1-24bec93a6123",
   "metadata": {},
   "source": [
    "In the Kaggle competition, I tried several methods. For embedding, I primarily used TF-IDF, though I also attempted using Word2Vec, but the process took too much time. Regarding model selection, I experimented with three machine learning models: Random Forest, Decision Tree, and XGBoost. Additionally, I also tried using deep learning methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc87ec9-782d-4db7-803d-bf7bb8a29a87",
   "metadata": {},
   "source": [
    "### Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3409a8f-b928-4ae2-a5af-8f6cddb315f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206e7f98-78fe-410e-966f-e0ee296b524c",
   "metadata": {},
   "source": [
    "***Data login & processing***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae4530c-b3b2-49d8-9704-0c2048b4a0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "with open('dm-2024-isa-5810-lab-2-homework/tweets_DM.json', 'r') as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line))\n",
    " \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868e4c13-ce96-4ba6-a352-d5f048af3524",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion = pd.read_csv('dm-2024-isa-5810-lab-2-homework/emotion.csv')\n",
    "data_identification = pd.read_csv('dm-2024-isa-5810-lab-2-homework/data_identification.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9888d1c5-b3ee-4979-9e3f-9adac24f6f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)\n",
    "_source = df['_source'].apply(lambda x: x['tweet'])\n",
    "df = pd.DataFrame({\n",
    "    'tweet_id': _source.apply(lambda x: x['tweet_id']),\n",
    "    'hashtags': _source.apply(lambda x: x['hashtags']),\n",
    "    'text': _source.apply(lambda x: x['text']),\n",
    "})\n",
    "df = df.merge(data_identification, on='tweet_id', how='left')\n",
    "\n",
    "train_data = df[df['identification'] == 'train']\n",
    "test_data = df[df['identification'] == 'test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bd56a9-5e9e-4648-9769-177e0eb9d76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.merge(emotion, on='tweet_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a97ef74-3eac-460c-a57a-ab7502463352",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.drop_duplicates(subset=['text'], keep=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af2b6b3-a92b-4b94-9af8-9b4e0324b0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437d0b9e-cf9c-4feb-9bc8-047836c14947",
   "metadata": {},
   "source": [
    "***Prepare data for training model***<br>\n",
    "Due to the large size of the dataset, I chose to sample a certain percentage of the data for training. Even so, using just 20% of the data still required about an hour of training. Once I found suitable parameter combinations, I used the entire dataset for training. While this approach is not entirely rigorous, it serves as a useful reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83ac35a-cede-4309-8a4c-5585c09a66de",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_sample = train_data.sample(frac=1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a271b472-7042-4ec1-8726-95e985031e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_data = train_data['emotion']\n",
    "X_train_data = train_data.drop(['tweet_id', 'emotion', 'identification', 'hashtags'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6cda08-8118-49dd-8d71-13eedddc357d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train_data, y_train_data, test_size=0.2, random_state=42, stratify=y_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32febd8-dc66-464a-88a1-3b6d47a0e9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(max_features=500)\n",
    "X = tfidf.fit_transform(X_train['text']).toarray()\n",
    "X_test = tfidf.transform(X_test['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77fcb69-bd87-4718-af18-61ed46c726d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y_train)\n",
    "y_test = le.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90dea69-df6f-4986-af7b-12ec4f988b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mapping = dict(zip(le.classes_, range(len(le.classes_))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1269399-68f0-4676-a39e-b70554549cf3",
   "metadata": {},
   "source": [
    "***Training model & Accuracy***<br>\n",
    "For the machine learning model selection, I tried XGBoost, Random Forest, and Decision Tree. Among them, XGBoost performed very poorly, producing outputs limited to only three emotion categories, resulting in poor outcomes. Initially, I determined that this was not a data issue, as the input was consistent across all models. Therefore, I decided not to use XGBoost as my model. As for Decision Tree and Random Forest, while Decision Tree trained faster, Random Forest produced better results. Ultimately, I chose to fine-tune Random Forest to find the best-performing model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b40fa92-f3bd-4968-829a-65e3e6cea5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Initialize the XGBoost classifier\n",
    "clf = XGBClassifier()\n",
    "# Fit the classifier to your data\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55320d1c-4073-4edb-ae3f-10904b471e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38acdced-0b9f-49da-92b4-f74b795b55c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters\n",
    "param_grid = {\n",
    "    'n_estimators': 100,\n",
    "    'max_depth': 20,\n",
    "    'min_samples_split': 5,\n",
    "    'min_samples_leaf': 3\n",
    "}\n",
    "\n",
    "# Train the RandomForestClassifier\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f13a7a9-68c0-40a1-bde3-f1f9b11bb179",
   "metadata": {},
   "source": [
    "***Try different combination of parameters***<br>\n",
    "I tried a total of five parameters for Random Forest, each with three different values, resulting in 3^5 combinations. Ultimately, I found that the combination of max_depth=20 and max_features=\"sqrt\" performed the best. However, the accuracy was around 0.44 for all combinations. The primary goal was to find a suitable set of parameters to avoid overfitting caused by a lack of parameter control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c72063-27b0-40f8-9336-028bf7b2ee0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV, ParameterGrid\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50],\n",
    "    'max_depth': [20],\n",
    "    'min_samples_split': [2],\n",
    "    'min_samples_leaf': [1],\n",
    "    'max_features': ['sqrt']\n",
    "}\n",
    "\n",
    "# Wrap the grid search process with tqdm\n",
    "def tqdm_grid_search(cv, estimator, param_grid, X, y, scoring='accuracy'):\n",
    "    param_list = list(ParameterGrid(param_grid))  # Generate all parameter combinations\n",
    "    results = []\n",
    "    for params in tqdm(param_list, desc=\"Grid Search Progress\"):\n",
    "        clf = estimator.set_params(**params)\n",
    "        scores = []\n",
    "        for train_idx, test_idx in cv.split(X, y):\n",
    "            X_train, X_test = X[train_idx], X[test_idx]\n",
    "            y_train, y_test = y[train_idx], y[test_idx]\n",
    "            clf.fit(X_train, y_train)\n",
    "            scores.append(clf.score(X_test, y_test))\n",
    "        results.append((params, scores))\n",
    "    return results\n",
    "\n",
    "# Example usage\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "\n",
    "# Define cross-validation strategy\n",
    "cv = StratifiedKFold(n_splits=3)\n",
    "\n",
    "# Call tqdm_grid_search\n",
    "results = tqdm_grid_search(\n",
    "    cv=cv,\n",
    "    estimator=RandomForestClassifier(),\n",
    "    param_grid=param_grid,\n",
    "    X=np.array(X),  # Convert to numpy array if needed\n",
    "    y=np.array(y),  # Convert to numpy array if needed\n",
    ")\n",
    "\n",
    "# Print results\n",
    "print(\"Best Parameters and Scores:\")\n",
    "for params, scores in results:\n",
    "    print(f\"Params: {params}, Mean Accuracy: {np.mean(scores):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63cce27-dedf-4563-911e-6b39c144d970",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "y_pred_train = clf.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e168174-e16d-4c51-9d6f-6931ac421241",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "print(accuracy_score(y, y_pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85540eb-3eb2-4ea9-80da-140668fe5c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "# Generate classification report\n",
    "report = classification_report(y_test, y_pred, target_names=le.classes_, digits=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e30b767-52f2-48a0-ae0e-ab4e7ee141eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55becd1d-9773-4dce-b54e-c222c5ee6fb2",
   "metadata": {},
   "source": [
    "### Deep learning \n",
    "I also tried using deep learning methods to train the model. Although the results were better than Random Forest, the training time was significantly longer. Therefore, I decided to first focus on fine-tuning the Random Forest model before adjusting the deep learning approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5ea627-b60f-4eb9-b8c0-5c4797191e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def label_encode(le, labels):\n",
    "    enc = le.transform(labels)\n",
    "    return keras.utils.to_categorical(enc)\n",
    "\n",
    "def label_decode(le, one_hot_label):\n",
    "    dec = np.argmax(one_hot_label, axis=1)\n",
    "    return le.inverse_transform(dec)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(y_train)\n",
    "\n",
    "y = label_encode(label_encoder, y_train)\n",
    "y_test = label_encode(label_encoder, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e4d076-d276-460a-a61e-488e0d1682ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I/O check\n",
    "input_shape = X.shape[1]\n",
    "print('input_shape: ', input_shape)\n",
    "\n",
    "output_shape = len(le.classes_)\n",
    "print('output_shape: ', output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50d0802-6b8b-4d60-8281-9914d99306ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa96c805-9543-4051-b7c9-511d02dfc118",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.layers import ReLU, Softmax\n",
    "\n",
    "# input layer\n",
    "model_input = Input(shape=(input_shape, ))  # 500\n",
    "x = model_input\n",
    "\n",
    "# 1st hidden layer\n",
    "X_W1 = Dense(units=64)(x)  # 64\n",
    "H1 = ReLU()(X_W1)\n",
    "\n",
    "# 2nd hidden layer\n",
    "H1_W2 = Dense(units=64)(H1)  # 64\n",
    "H2 = ReLU()(H1_W2)\n",
    "\n",
    "# output layer\n",
    "H2_W3 = Dense(units=output_shape)(H2)  # 4\n",
    "H3 = Softmax()(H2_W3)\n",
    "\n",
    "model_output = H3\n",
    "\n",
    "# create model\n",
    "model = Model(inputs=[model_input], outputs=[model_output])\n",
    "\n",
    "# loss function & optimizer\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# show model construction\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23e96d2-eef2-4b06-bbc5-66515ebc45b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410ee18f-286c-4638-8de1-d3308048d7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import CSVLogger\n",
    "\n",
    "csv_logger = CSVLogger('logs/training_log.csv')\n",
    "\n",
    "# training setting\n",
    "epochs = 4\n",
    "batch_size = 32\n",
    "\n",
    "# training!\n",
    "history = model.fit(X, y, \n",
    "                    epochs=epochs, \n",
    "                    batch_size=batch_size, \n",
    "                    callbacks=[csv_logger],\n",
    "                    validation_data = (X_test, y_test))\n",
    "print('training finish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ebeb96-d971-4236-9d51-42e57a99e3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's take a look at the training log\n",
    "training_log = pd.DataFrame()\n",
    "training_log = pd.read_csv(\"logs/training_log.csv\")\n",
    "training_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709e24a8-a170-4c99-ac4f-c804fffa7c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer here\n",
    "df = pd.DataFrame(training_log)\n",
    "plt.figure(figsize=(14, 6))\n",
    "# Subplot 1: Accuracy\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(df['epoch'], df['accuracy'], label='Accuracy', marker='o')\n",
    "plt.plot(df['epoch'], df['val_accuracy'], label='Validation Accuracy', marker='o')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "# Subplot 2: Loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(df['epoch'], df['loss'], label='Loss', marker='o')\n",
    "plt.plot(df['epoch'], df['val_loss'], label='Validation Loss', marker='o')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "# Show the plots\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22fab01d-15f7-499e-b32e-f61972d14ab3",
   "metadata": {},
   "source": [
    "### Use OpenAI embedding\n",
    "I originally planned to use OpenAI's API for embedding, but I found that it required a lot of time—processing 300K records would take over 3 hours. Therefore, I determined that this method was not suitable for my current situation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dceb7bdc-c244-4eb9-9560-55a1de890770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example data (replace with your actual data)\n",
    "X_train_texts = X_train['text'].tolist()  # Convert training text column to a list\n",
    "X_test_texts = X_test['text'].tolist()    # Convert testing text column to a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5484ce-91d3-40b3-b9b7-050514e1bcba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import numpy as np\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from tqdm import tqdm  # For showing progress bars\n",
    "import getpass\n",
    "import os\n",
    "\n",
    "# Set OpenAI API Key\n",
    "API_Key = \"\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")\n",
    "client = OpenAI(\n",
    "  api_key=os.environ['OPENAI_API_KEY'],  # this is also the default, it can be omitted\n",
    ")\n",
    "\n",
    "# Function to generate embeddings from OpenAI\n",
    "def generate_embeddings(texts, model=\"text-embedding-ada-002\"):\n",
    "    embeddings = []\n",
    "    for text in tqdm(texts, desc=\"Generating embeddings\"):\n",
    "        response = client.embeddings.create(input=text, model=model)\n",
    "        embedding = response.data[0].embedding\n",
    "        embeddings.append(embedding)\n",
    "    return np.array(embeddings)\n",
    "\n",
    "# Generate embeddings for train and test data\n",
    "train_embeddings = generate_embeddings(X_train_texts)\n",
    "test_embeddings = generate_embeddings(X_test_texts)\n",
    "\n",
    "# Reduce dimensions to 500 using Truncated SVD\n",
    "'''svd = TruncatedSVD(n_components=500, random_state=42)\n",
    "train_embeddings_500 = svd.fit_transform(train_embeddings)\n",
    "test_embeddings_500 = svd.transform(test_embeddings)\n",
    "\n",
    "# Train embeddings are now reduced to 500 dimensions\n",
    "print(\"Shape of Train Embeddings:\", train_embeddings_500.shape)\n",
    "print(\"Shape of Test Embeddings:\", test_embeddings_500.shape)'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9a9b82-2076-4a97-a369-6908aa0457db",
   "metadata": {},
   "source": [
    "### Generate Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d724549d-2f0d-48d1-a665-13ab393bace2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_data = test_data.drop(['tweet_id', 'identification', 'hashtags'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1e283f-9e06-4181-8528-8ce0ebaa1b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_data = tfidf.transform(X_test_data['text']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d435b049-c2bf-4989-bfc1-25e04c057c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deep learning\n",
    "y_test_pred = model.predict(X_test_data, batch_size=128)\n",
    "y_pred_labels = label_decode(label_encoder, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d6fa0e-d901-4644-8bce-d2f3fcbfa0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315c1b14-32ff-4f60-b2d6-d5763c46f89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# machine learning\n",
    "y_test_pred = clf.predict(X_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151298d5-3b9c-4f9d-92e2-4c51cac184db",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_labels = le.inverse_transform(y_test_pred)\n",
    "y_pred_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4a3053-6645-425b-836f-274136b561ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\n",
    "    'id': test_data['tweet_id'],\n",
    "    'emotion': y_pred_labels\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d5f705-1c5a-48d6-977e-4d3cb6f7c766",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('kaggle/submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758db5ba-c469-44d8-8014-374c1fa2e762",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
